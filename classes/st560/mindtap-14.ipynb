{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MindTap 14 - Simple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.1 - Simple Linear Regression Model\n",
    "\n",
    "### Regression Model and Regression Equation\n",
    "\n",
    "Regression analysis requires us to develop a **regression model**, an equation showing how a dependent variable $ y $ is related to the independent variable $ x $.\n",
    "\n",
    "The simple linear regression model is as follows:\n",
    "\n",
    "$$\n",
    "y = \\beta _0 + \\beta _1 x + \\epsilon\n",
    "$$\n",
    "\n",
    "Where $ \\beta _0 $ and $ \\beta _1 $ are referred to as the parameters of the model, and $ \\epsilon $ is a random variable called the error term. The error term accounts for the variability in y cannot be explained by the linear relationship between $ x $ and $ y $.\n",
    "\n",
    "The equation that describes how the expected value of $ y $, denoted $ E(y) $, is related to $ x $ is called the **regression equation**:\n",
    "\n",
    "$$\n",
    "E(y) = \\beta _0 + \\beta _1 x\n",
    "$$\n",
    "\n",
    "Note that this model is a line with slope $ \\beta _1 $ and y-intercept $ \\beta _0 $.\n",
    "\n",
    "### Estimated Regression Equation\n",
    "\n",
    "Since the values of the population parameters $ \\beta _0 $ and $ \\beta _1 $ are rarely known, they must be estimated. These estimates are represented as the sample statistics $ b_0 $ and $ b_1 $. With this, we can now construct the **estimated regression equation**:\n",
    "\n",
    "$$\n",
    "\\hat{y} = b_0 + b_1 x\n",
    "$$\n",
    "\n",
    "The graph of this equation is known as the *estimated regression line*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.2 Least Squares Method\n",
    "\n",
    "The **least squares method** is a procedure for using sample data to find the estimated regression equation. This method uses sample data to provide the values of $b_0$ and $b_1$ that minimize the *sum of the squares of the deviations* between the observed values of the dependent variable $y_i$ and the predicted values of the dependent variable $\\hat{y}_i$:\n",
    "\n",
    "$$\n",
    "min\\ \\Sigma (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "where\n",
    "- $y_i$ = observed value of the dependent variable for the *i*th observation\n",
    "- $\\hat{y}_i$ = predicted value of the dependent variable for the *i*th observation.\n",
    "\n",
    "Using calculus, you can derive the following equations for $b_0$ and $b_1$:\n",
    "\n",
    "$$\n",
    "b_1 = \\frac{\\Sigma (x_i - \\bar{x})(y_i - \\bar{y})}{\\Sigma (x_i - \\bar{x})^2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b_0 = \\bar{y} - b_1 \\bar{x}\n",
    "$$\n",
    "\n",
    "where\n",
    "- $x_i$ = value of the independent variable for the *i*th observation\n",
    "- $y_i$ = value of the dependent variable for the *i*th observation\n",
    "- $\\bar{x}$ = mean value for the independent variable\n",
    "- $\\bar{y}$ = mean value for the dependent variable\n",
    "- $n$ = total number of observations"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
