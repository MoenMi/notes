{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MindTap 14 - Simple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.1 Simple Linear Regression Model\n",
    "\n",
    "### Regression Model and Regression Equation\n",
    "\n",
    "Regression analysis requires us to develop a **regression model**, an equation showing how a dependent variable $ y $ is related to the independent variable $ x $.\n",
    "\n",
    "The simple linear regression model is as follows:\n",
    "\n",
    "$$\n",
    "y = \\beta _0 + \\beta _1 x + \\epsilon\n",
    "$$\n",
    "\n",
    "Where $ \\beta _0 $ and $ \\beta _1 $ are referred to as the parameters of the model, and $ \\epsilon $ is a random variable called the error term. The error term accounts for the variability in y cannot be explained by the linear relationship between $ x $ and $ y $.\n",
    "\n",
    "The equation that describes how the expected value of $ y $, denoted $ E(y) $, is related to $ x $ is called the **regression equation**:\n",
    "\n",
    "$$\n",
    "E(y) = \\beta _0 + \\beta _1 x\n",
    "$$\n",
    "\n",
    "Note that this model is a line with slope $ \\beta _1 $ and y-intercept $ \\beta _0 $.\n",
    "\n",
    "### Estimated Regression Equation\n",
    "\n",
    "Since the values of the population parameters $ \\beta _0 $ and $ \\beta _1 $ are rarely known, they must be estimated. These estimates are represented as the sample statistics $ b_0 $ and $ b_1 $. With this, we can now construct the **estimated regression equation**:\n",
    "\n",
    "$$\n",
    "\\hat{y} = b_0 + b_1 x\n",
    "$$\n",
    "\n",
    "The graph of this equation is known as the *estimated regression line*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.2 Least Squares Method\n",
    "\n",
    "The **least squares method** is a procedure for using sample data to find the estimated regression equation. This method uses sample data to provide the values of $b_0$ and $b_1$ that minimize the *sum of the squares of the deviations* between the observed values of the dependent variable $y_i$ and the predicted values of the dependent variable $\\hat{y}_i$:\n",
    "\n",
    "$$\n",
    "min\\ \\Sigma (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "where\n",
    "- $y_i$ = observed value of the dependent variable for the *i*th observation\n",
    "- $\\hat{y}_i$ = predicted value of the dependent variable for the *i*th observation.\n",
    "\n",
    "Using calculus, you can derive the following equations for $b_0$ and $b_1$:\n",
    "\n",
    "$$\n",
    "b_1 = \\frac{\\Sigma (x_i - \\bar{x})(y_i - \\bar{y})}{\\Sigma (x_i - \\bar{x})^2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b_0 = \\bar{y} - b_1 \\bar{x}\n",
    "$$\n",
    "\n",
    "where\n",
    "- $x_i$ = value of the independent variable for the *i*th observation\n",
    "- $y_i$ = value of the dependent variable for the *i*th observation\n",
    "- $\\bar{x}$ = mean value for the independent variable\n",
    "- $\\bar{y}$ = mean value for the dependent variable\n",
    "- $n$ = total number of observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.3 Coefficient of Determination\n",
    "\n",
    "The **coefficient of determination** provides a measure of the goodness of the fit for an estimated regression equation.\n",
    "\n",
    "For the *i*th observation, the difference between the observed value of the dependent variable ($y_i$) and the predicted value of the dependent variable ($\\hat{y}_i$) is called ***i*th residual** ($y_i - \\hat{y}_i$). The *i*th residual represents the error in using $\\hat{y}_i$ to estimate $y_i$. The sum of squares of these residuals or errors is the quantity that is minimized in the least squares method. This quantity is known as the **sum of squares due to error (SSE)**.\n",
    "\n",
    "$$\n",
    "SSE = \\Sigma (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "The value of SSE is a measure of the errror in using the estimated regression equation to predict the value of the dependent variable in a sample.\n",
    "\n",
    "When we don't know the size of the population, we must also estimate the population mean with the sample mean. The error due to this can be calculated as the **total sum of squares (SST)**.\n",
    "\n",
    "$$\n",
    "SST = \\Sigma (y_i - \\bar{y})^2\n",
    "$$\n",
    "\n",
    "Note that we can think of SSE as a measure of how well the observations cluster above the line $ y=\\hat{y} $ and SST as a measure of how well the observations cluster above the line $ y=\\bar{y} $.\n",
    "\n",
    "The **sum of squares due to regression (SSR)** is a measure of how much the values of $\\hat{y}$ deviate from $\\bar{y}$:\n",
    "\n",
    "$$\n",
    "SSR = \\Sigma (\\hat{y}_i - \\bar{y})^2\n",
    "$$\n",
    "\n",
    "Theses 3 values are related as such:\n",
    "\n",
    "$$\n",
    "SST = SSR + SSE\n",
    "$$\n",
    "\n",
    "The **coefficient of determination ($r^2$)** measures the goodness of fit for the estimated regression equation. Note that $r^2$ will always have a value between 0 and 1.\n",
    "\n",
    "$$\n",
    "r^2 = \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Coefficient\n",
    "\n",
    "The **correlation coefficient ($r_{xy}$)** is a descriptive measure of the strength of linear association between two variables $x$ and $y$. Note that the values of $r_{xy}$ lie between -1 and 1 where negative values of $r_{xy}$ indicate a negative relationship between $x$ and $y$ and positive and positive values of $r_{xy}$ indicate a positive relationship between $x$ and $y$.\n",
    "\n",
    "$$\n",
    "r_{xy} = (sign\\ of\\ b_1)\\sqrt{r^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.4 Model Assumptions\n",
    "\n",
    "The tests of significance in regression analysis are based off on the following assumptions of the error term $\\epsilon$:\n",
    "\n",
    "1. $E(\\epsilon) = 0$. This is necessary so that the expected value of the simple linear regression model $y = \\beta _0 + \\beta _1 x + \\epsilon$ works itself out to be $y = \\beta _0 + \\beta _1 x$.\n",
    "\n",
    "2. The variance of $\\epsilon$, denoted by $\\sigma^2$, is the same for all values of $x$.\n",
    "\n",
    "3. The values of $\\epsilon$ are independent.\n",
    "\n",
    "4. The error term $\\epsilon$ is a normally distributed random variable for all values of $x$.\n",
    "\n",
    "Note that linear regression also assumes that relationships between the variables analyzed is in fact linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.5 Testing for Signifance"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
