{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 18 - Least Squares Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18.1 - Problem Formulation\n",
    "\n",
    "Given a matrix $A \\in \\mathbb{R}^{m \\times n}$ (with $m \\geq n$) and a vector $\\mathbf{b} \\in \\mathbb{R}^m$, we are interested in finding a vector $x \\in \\mathbb{R}^n$ such that $A\\mathbf{x} = \\mathbf{b}$ if possible, or at least $A\\mathbf{x} \\approx \\mathbf{b}$.\n",
    "\n",
    "In the case where no such $\\mathbf{x}$ satisfies the equation exactly, we define the residual as $r(\\mathbf{x}) = \\mathbf{b} - A\\mathbf{x}$. The function $\\phi(\\mathbf{x})$ represents the square of the 2-norm of the residual\n",
    "\n",
    "$$ \\phi(\\mathbf{x}) = ||\\mathbf{b} - A\\mathbf{x}||^2 = (\\mathbf{b} - A\\mathbf{x})^T (\\mathbf{b} - A\\mathbf{x}) $$\n",
    "\n",
    "Our goal is to find a vector $\\mathbf{x}$ which makes $A\\mathbf{x}$ very close to $\\mathbf{b}$, i.e., the $\\mathbf{x}$ that minimizes the function $\\phi(\\mathbf{x})$.\n",
    "\n",
    "The first necessary condition for minimization (setting the gradient to zero) leads to the normal equations\n",
    "\n",
    "$$ \\nabla \\phi(\\mathbf{x}) = 2A^T (\\mathbf{b} - A\\mathbf{x}) = 0 $$\n",
    "\n",
    "which implies\n",
    "\n",
    "$$ A^T A \\mathbf{x} = A^T \\mathbf{b} $$\n",
    "\n",
    "This provides a closed-form solution $\\mathbf{x}^* = (A^T A)^{-1} A^T \\mathbf{b}$, assuming $A^T A$ is invertible. The Hessian matrix, given by $2A^T A$, is positive semi-definite, indicating that our solution is indeed a minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18.2 - Singular Value Decomposition for Least Squares\n",
    "\n",
    "Any matrix $A \\in \\mathbb{R}^{m \\times n}$ can be decomposed using the singular value decomposition (SVD), expressed as $A = U \\Sigma V^T$. Here, $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal matrices, and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ is a diagonal matrix with non-negative real numbers on its diagonal.\n",
    "\n",
    "For practical computations, particularly in the context of least squares problems, we often use the reduced form of SVD: $A = U_r \\Sigma_r V^T_r$, where $U_r \\in \\mathbb{R}^{m \\times r}$, $V_r \\in \\mathbb{R}^{n \\times r}$, and $\\Sigma_r \\in \\mathbb{R}^{r \\times r}$ correspond to the first $r$ singular values, with $r$ being the rank of $A$. The least squares solution, then, is efficiently computed as\n",
    "\n",
    "$$ \\mathbf{x} \\approx V_r \\Sigma_r^{-1} U_r^T \\mathbf{b} $$\n",
    "\n",
    "where $\\Sigma_r^{-1}$ is the diagonal matrix formed by taking the reciprocal of the non-zero elements in $\\Sigma_r$.\n",
    "\n",
    "Employing SVD to solve the least squares problem is particularly advantageous due to its numerical stability and its effectiveness in handling matrices that are rank-deficient or close to rank-deficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Least squares solution x: [ 0.33333333 -0.33333333]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the matrix A and vector b\n",
    "A = np.array([[2, 0], [-1, 1], [0, 2]])\n",
    "b = np.array([1, 0, -1])\n",
    "\n",
    "# Perform Singular Value Decomposition\n",
    "# Use reduced SVD with option full_matrices=False\n",
    "U, sigma, VT = np.linalg.svd(A, full_matrices=False)\n",
    "\n",
    "# Create a diagonal matrix of the inverses of the singular values\n",
    "Sigma_inv = np.diag(1 / sigma)\n",
    "\n",
    "# Compute the pseudo-inverse of A\n",
    "A_pinv = VT.T @ Sigma_inv @ U.T\n",
    "\n",
    "# Compute the least squares solution using the pseudo-inverse\n",
    "x = A_pinv @ b\n",
    "\n",
    "print(\"Least squares solution x:\", x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18.3 - Gradient Descent for Least Squares\n",
    "\n",
    "Given a matrix $A \\in \\mathbb{R}^{m \\times n}$ and a vector $\\mathbf{b} \\in \\mathbb{R}^m$, we aim to find a vector $\\mathbf{x} \\in \\mathbb{R}^n$ that minimizes the objective function:\n",
    "\n",
    "$$ \\phi(\\mathbf{x}) = ||A\\mathbf{x} - \\mathbf{b}||^2 $$\n",
    "\n",
    "The gradient of the objective function $\\phi$ with respect to $\\mathbf{x}$ is given by\n",
    "\n",
    "$$ \\nabla \\phi(\\mathbf{x}) = 2A^T (A\\mathbf{x} - \\mathbf{b}) $$\n",
    "\n",
    "**Gradient Descent Algorithm**\n",
    "- **Initialization**: Start with an initial guess for $\\mathbf{x}$, typically $\\mathbf{x}_0 = 0$ or a random vector.\n",
    "- **Iteration**: Update $\\mathbf{x}$ iteratively using the formula:\n",
    "\n",
    "    $$ \\mathbf{x}_{\\text{new}} = \\mathbf{x}_{\\text{old}} - \\alpha \\nabla \\phi(\\mathbf{x}_{\\text{old}}) $$\n",
    "\n",
    "    where $\\alpha$ is the learning rate, a small positive scalar that controls the step size.\n",
    "- **Convergence Check**: Repeat the iteration until $\\nabla \\phi(\\mathbf{x})$ is sufficiently small or a maximum number of iterations is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution x: [ 0.33333328 -0.33333328]\n"
     ]
    }
   ],
   "source": [
    "# Parameters for gradient descent\n",
    "alpha = 0.01        # Learning rate\n",
    "max_iter = 1000     # Max number of iterations\n",
    "tolerance = 1e-6    # Convergence tolerance\n",
    "\n",
    "# Initial guess for x\n",
    "x = np.zeros(A.shape[1])\n",
    "\n",
    "# Gradient descent iteration\n",
    "for i in range(max_iter):\n",
    "    gradient = 2 * A.T @ (A @ x - b)\n",
    "    x -= alpha * gradient\n",
    "    if np.linalg.norm(gradient) < tolerance:\n",
    "        break\n",
    "\n",
    "print(\"Solution x:\", x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "math311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
